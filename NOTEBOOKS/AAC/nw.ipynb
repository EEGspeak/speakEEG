{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NW AAC\n",
    "Next-word prediction for AAC board using Gensim and Word2Vec\n",
    "\n",
    "Uses `contractions` to remove contractions from data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import re\n",
    "import contractions\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hi, how are you doing?', \"i'm fine. how about yourself?\", \"i'm pretty good. thanks for asking.\", 'no problem. so how have you been?', \"i've been great. what about you?\"]\n",
      "3725 3725\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>hi, how are you doing?</td>\n",
       "      <td>i'm fine. how about yourself?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>i'm fine. how about yourself?</td>\n",
       "      <td>i'm pretty good. thanks for asking.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>i'm pretty good. thanks for asking.</td>\n",
       "      <td>no problem. so how have you been?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>no problem. so how have you been?</td>\n",
       "      <td>i've been great. what about you?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>i've been great. what about you?</td>\n",
       "      <td>i've been good. i'm in school right now.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   num                             question  \\\n",
       "0    0               hi, how are you doing?   \n",
       "1    1        i'm fine. how about yourself?   \n",
       "2    2  i'm pretty good. thanks for asking.   \n",
       "3    3    no problem. so how have you been?   \n",
       "4    4     i've been great. what about you?   \n",
       "\n",
       "                                     answer  \n",
       "0             i'm fine. how about yourself?  \n",
       "1       i'm pretty good. thanks for asking.  \n",
       "2         no problem. so how have you been?  \n",
       "3          i've been great. what about you?  \n",
       "4  i've been good. i'm in school right now.  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read data, convert to lists\n",
    "df = pd.read_csv(\"Conversation.csv\")\n",
    "\n",
    "qlist = df[\"question\"].tolist()\n",
    "alist = df[\"answer\"].tolist()\n",
    "print(qlist[:5])\n",
    "print(len(qlist), len(alist))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3725 3725\n",
      "['poodles bark a lot.', 'they sure do.', 'they bark at everything.', 'they never shut up.', 'why did you get a poodle?', \"it is my mom's dog.\", 'so she likes poodles.']\n",
      "['they sure do.', 'they bark at everything.', 'they never shut up.', 'why did you get a poodle?', \"it is my mom's dog.\", 'so she likes poodles.', 'she says they are good watchdogs.']\n",
      "['we have not been in a while.', 'we have not been in a month.', 'the last time we went, you almost drowned.', 'no, i did not.', 'then why did the lifeguard dive into the water?', 'i think he wanted to cool off.', 'he swam right up to you.']\n",
      "['we have not been in a month.', 'the last time we went, you almost drowned.', 'no, i did not.', 'then why did the lifeguard dive into the water?', 'i think he wanted to cool off.', 'he swam right up to you.', 'and then he turned right around.']\n"
     ]
    }
   ],
   "source": [
    "# remove contractions\n",
    "qlist = [contractions.fix(l) for l in qlist]\n",
    "alist = [contractions.fix(l) for l in alist]\n",
    "\n",
    "print(len(qlist), len(alist))\n",
    "\n",
    "print(qlist[765:772])\n",
    "print(alist[765:772])\n",
    "\n",
    "# does not remove possessives, though\n",
    "print(qlist[785:792])\n",
    "print(alist[785:792])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3725 3725\n",
      "['poodles bark a lot', 'they sure do', 'they bark at everything', 'they never shut up', 'why did you get a poodle', \"it is my mom's dog\", 'so she likes poodles']\n",
      "['they sure do', 'they bark at everything', 'they never shut up', 'why did you get a poodle', \"it is my mom's dog\", 'so she likes poodles', 'she says they are good watchdogs']\n",
      "['we have not been in a while', 'we have not been in a month', 'the last time we went you almost drowned', 'no i did not', 'then why did the lifeguard dive into the water', 'i think he wanted to cool off', 'he swam right up to you']\n",
      "['we have not been in a month', 'the last time we went you almost drowned', 'no i did not', 'then why did the lifeguard dive into the water', 'i think he wanted to cool off', 'he swam right up to you', 'and then he turned right around']\n"
     ]
    }
   ],
   "source": [
    "# remove punctuation, numbers, signs. except for remaining apostrophes\n",
    "qlist = [re.sub(r\"[^a-zA-Z\\s']\", \"\", l) for l in qlist]\n",
    "alist = [re.sub(r\"[^a-zA-Z\\s']\", \"\", l) for l in alist]\n",
    "\n",
    "print(len(qlist), len(alist))\n",
    "\n",
    "print(qlist[765:772])\n",
    "print(alist[765:772])\n",
    "\n",
    "print(qlist[785:792])\n",
    "print(alist[785:792])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7450\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['hi', 'how', 'are', 'you', 'doing'],\n",
       " ['i', 'am', 'fine', 'how', 'about', 'yourself'],\n",
       " ['i', 'am', 'pretty', 'good', 'thanks', 'for', 'asking'],\n",
       " ['no', 'problem', 'so', 'how', 'have', 'you', 'been'],\n",
       " ['i', 'have', 'been', 'great', 'what', 'about', 'you']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combine to compatible sentences nested lists (each word becomes indiv element of parent list)\n",
    "sentences = list(map(lambda x : x.split(), qlist))\n",
    "sentences.extend(map(lambda x : x.split(), alist))\n",
    "\n",
    "print(len(sentences))\n",
    "sentences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1003 300\n"
     ]
    }
   ],
   "source": [
    "# put into W2V model & save\n",
    "model = Word2Vec(sentences, vector_size=300, window=5, workers=4, epochs=10, min_count=5)\n",
    "model.save(\"conv.model\")\n",
    "\n",
    "pretrained_weights = model.wv.vectors\n",
    "vocab_size, embedding_size = pretrained_weights.shape\n",
    "print(vocab_size, embedding_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average context's embed vectors and find most similar vector for next word\n",
    "class WordPredictor:\n",
    "    def __init__(self, sentences: list[list[str]], method: str = \"word2vec\", max_ngram: int = 5):\n",
    "        \"\"\"\n",
    "        Initializes the predictor with training sentences\n",
    "\n",
    "        Args:\n",
    "            sentences: List of lists, inevitably loaded to W2V model\n",
    "            method: Prediction model specification (\"word2vec\", \"ngram\", or \"lstm\")\n",
    "            max_ngram: Maximum n-gram lookback size\n",
    "        \n",
    "        Methods:\n",
    "            build_vocabulary: creates a word-to-index mapping from training sentences\n",
    "            build_ngram_models: constructs multiple N-gram models with different context windows\n",
    "            predict_next_word: predicts next word given current context\n",
    "            prepare_lstm_data: prepares training data for LSTM model\n",
    "            build_lstm_model: initializes and trains LSTM model\n",
    "            predict_lstm: predicts next word using trained LSTM model\n",
    "        \"\"\"\n",
    "        self.method = method\n",
    "        self.sentences = sentences\n",
    "        self.vocab = self.build_vocabulary()\n",
    "\n",
    "        if method == \"word2vec\":\n",
    "            self.model = Word2Vec(\n",
    "                sentences, vector_size=100, window=5, min_count=1, workers=4\n",
    "            )\n",
    "\n",
    "        elif method == \"lstm\":\n",
    "            self.prepare_lstm_data()\n",
    "            self.build_lstm_model()\n",
    "\n",
    "        else:\n",
    "            self.max_ngram = max_ngram\n",
    "            self.build_ngram_models()\n",
    "    \n",
    "    def build_vocabulary(self):\n",
    "        \"\"\" Build vocabulary from sentences \"\"\"\n",
    "\n",
    "        words = []\n",
    "        for sentence in sentences:\n",
    "            words.extend(sentence)\n",
    "\n",
    "        unique_words = set(words)\n",
    "\n",
    "        vocab = {word: idx for idx, word in enumerate(unique_words)}\n",
    "        \n",
    "        return vocab\n",
    "\n",
    "    def build_ngram_models(self):\n",
    "        \"\"\"\n",
    "        Build multiple N-gram models from the training sentences\n",
    "        Implements backoff in case phrase not found in training data\n",
    "        \"\"\"\n",
    "        self.ngram_models = {}\n",
    "\n",
    "        for n in range(self.max_ngram, 1, -1):\n",
    "            self.ngram_models[n] = {} # access each model using its context size\n",
    "\n",
    "            for sentence in self.sentences:\n",
    "                for i in range(len(sentence) - n + 1): # iterate through all context windows of length `n`\n",
    "                    prefix = tuple(sentence[i:i+n-1])\n",
    "                    next_word = sentence[i+n-1]\n",
    "                    if prefix not in self.ngram_models[n]:\n",
    "                        self.ngram_models[n][prefix] = Counter()\n",
    "                    self.ngram_models[n][prefix][next_word] += 1\n",
    "\n",
    "\n",
    "    def predict_next_word(self, context, top_n=5):\n",
    "        \"\"\"\n",
    "        Predict the next word given a context\n",
    "\n",
    "        Args:\n",
    "            context: List of words\n",
    "            top_n: # of predictions to return\n",
    "        \n",
    "        Returns:\n",
    "            List of (word, score) tuples\n",
    "        \"\"\"\n",
    "\n",
    "        if self.method == \"word2vec\":\n",
    "            context_vectors = [self.model.wv[word] for word in context]\n",
    "            avg_vector = np.mean(context_vectors, axis=0)\n",
    "\n",
    "            similar_words = self.model.wv.similar_by_vector(avg_vector, topn=top_n)\n",
    "\n",
    "            return similar_words\n",
    "\n",
    "        elif self.method == \"lstm\":\n",
    "            return self.predict_lstm(context, top_n)\n",
    "        \n",
    "        else: # method = N-gram w/ backoff\n",
    "            for n in range(self.max_ngram, 1, -1): # find the largest N-gram that works\n",
    "                if len(context) >= n - 1:\n",
    "                    prefix = tuple(context[-(n-1):])\n",
    "                    if prefix in self.ngram_models[n]:\n",
    "                        predictions = self.ngram_models[n][prefix].most_common(top_n)\n",
    "                        total = sum(self.ngram_models[n][prefix].values())\n",
    "                        return [(word, count / total) for word, count in predictions]\n",
    "            \n",
    "            # worst case just return most common words (unigram)\n",
    "            if 2 in self.ngram_models:\n",
    "                all_words = Counter()\n",
    "                for ngram_dict in self.ngram_models[2].values():\n",
    "                    all_words.update(ngram_dict)\n",
    "                total = sum(all_words.values())\n",
    "                return [(word, count / total) for word, count in all_words.most_common(top_n)]\n",
    "            \n",
    "            return [] # NULLGRAM (empty training dataset)\n",
    "    \n",
    "    def prepare_lstm_data(self):\n",
    "        \"\"\"\n",
    "        Prepare data for LSTM training\n",
    "        \"\"\"\n",
    "\n",
    "        self.word_to_idx = {word: idx for idx, word in enumerate(self.vocab)}\n",
    "        self.idx_to_word = {idx: word for word, idx in self.word_to_idx.items()}\n",
    "        self.vocab_size = len(self.vocab)\n",
    "\n",
    "        self.sequence_length = 5 # context window size\n",
    "        self.X = []\n",
    "        self.y = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "wp = WordPredictor(sentences, method=\"ngram\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('go', 0.22727272727272727), ('get', 0.13636363636363635), ('play', 0.09090909090909091), ('be', 0.06818181818181818), ('do', 0.045454545454545456)]\n",
      "i feel\n",
      "i feel sorry\n",
      "i feel sorry for\n",
      "i feel sorry for you\n",
      "i feel sorry for you and\n",
      "i feel sorry for you and you\n",
      "i feel sorry for you and you will\n",
      "i feel sorry for you and you will see\n",
      "i feel sorry for you and you will see the\n",
      "i feel sorry for you and you will see the collision\n",
      "i feel sorry for you and you will see the collision if\n",
      "i feel sorry for you and you will see the collision if they\n",
      "i feel sorry for you and you will see the collision if they crash\n",
      "i feel sorry for you and you will see the collision if they crash into\n",
      "i feel sorry for you and you will see the collision if they crash into the\n",
      "i feel sorry for you and you will see the collision if they crash into the house\n",
      "i feel sorry for you and you will see the collision if they crash into the house at\n",
      "i feel sorry for you and you will see the collision if they crash into the house at the\n",
      "i feel sorry for you and you will see the collision if they crash into the house at the end\n",
      "i feel sorry for you and you will see the collision if they crash into the house at the end of\n",
      "i feel sorry for you and you will see the collision if they crash into the house at the end of the\n",
      "i feel sorry for you and you will see the collision if they crash into the house at the end of the game\n",
      "i feel sorry for you and you will see the collision if they crash into the house at the end of the game on\n",
      "i feel sorry for you and you will see the collision if they crash into the house at the end of the game on friday\n",
      "i feel sorry for you and you will see the collision if they crash into the house at the end of the game on friday the\n",
      "i feel sorry for you and you will see the collision if they crash into the house at the end of the game on friday the thirteenth\n",
      "i feel sorry for you and you will see the collision if they crash into the house at the end of the game on friday the thirteenth floor\n",
      "i feel sorry for you and you will see the collision if they crash into the house at the end of the game on friday the thirteenth floor step\n",
      "i feel sorry for you and you will see the collision if they crash into the house at the end of the game on friday the thirteenth floor step on\n",
      "i feel sorry for you and you will see the collision if they crash into the house at the end of the game on friday the thirteenth floor step on it\n",
      "i feel sorry for you and you will see the collision if they crash into the house at the end of the game on friday the thirteenth floor step on it and\n",
      "i feel sorry for you and you will see the collision if they crash into the house at the end of the game on friday the thirteenth floor step on it and then\n",
      "i feel sorry for you and you will see the collision if they crash into the house at the end of the game on friday the thirteenth floor step on it and then spit\n",
      "i feel sorry for you and you will see the collision if they crash into the house at the end of the game on friday the thirteenth floor step on it and then spit on\n",
      "i feel sorry for you and you will see the collision if they crash into the house at the end of the game on friday the thirteenth floor step on it and then spit on it\n",
      "i feel sorry for you and you will see the collision if they crash into the house at the end of the game on friday the thirteenth floor step on it and then spit on it and\n",
      "i feel sorry for you and you will see the collision if they crash into the house at the end of the game on friday the thirteenth floor step on it and then spit on it and then\n",
      "i feel sorry for you and you will see the collision if they crash into the house at the end of the game on friday the thirteenth floor step on it and then spit on it and then spit\n",
      "i feel sorry for you and you will see the collision if they crash into the house at the end of the game on friday the thirteenth floor step on it and then spit on it and then spit on\n",
      "i feel sorry for you and you will see the collision if they crash into the house at the end of the game on friday the thirteenth floor step on it and then spit on it and then spit on it\n"
     ]
    }
   ],
   "source": [
    "response = wp.predict_next_word([\"i\", \"feel\", \"like\", \"i\", \"want\", \"to\"])\n",
    "print(response)\n",
    "\n",
    "wl = [\"i\", \"feel\"]\n",
    "\n",
    "for i in range(40):\n",
    "    print(\" \".join(wl))\n",
    "    n = wp.predict_next_word(wl)[0][0]\n",
    "    wl.append(n)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "speakeeg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
